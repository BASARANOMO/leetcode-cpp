# 458. Poor Pigs

## Solution1

Information theory: entropy

其实香农已经在《信息论》（信息熵）中给过我们结论了——我们一共可以进行n轮实验（n = minutesToTest / minutesToDie）：

经过所有实验，一只小猪能有多少种状态？第一轮就死、第二轮死、...、第n轮死，以及生还，所以一共有n + 1种状态
n + 1种状态所携带的信息为log_2(n + 1)比特，这也是一只小猪最多提供的信息量
而”buckets瓶液体中哪一瓶是毒“这件事，也有buckets种可能性，所以需要的信息量是log_2(buckets)
注：以上所有事件、状态都是先验等概的，所以可以直接对2取对数得到信息熵

因此一定存在一种“合理设计”的实验，使得我们只要有k只猪猪：满足 k * log_2(n + 1) >= log_2(buckets)时，则我们一定能得到足够的信息量去判断哪一瓶是毒。

至于怎么设计实验，香农没说。就像费马当年写不下的那份证明一样，大道至简。

--- update ---

“信息熵相等”意味着什么似乎引起了很多人的讨论。有许多同学认为，猪猪所带来的信息量多于毒瓶子的信息量，并不代表一定有这样的“实验方案”：即k * log_2(n + 1) >= log_2(buckets) 是 “能找到实验方案”的必要不充分条件。

其实我们可以从另一个角度看这个问题。信息熵只是对信息的度量，直观层面我们可以用“状态的数量”取看待一个事件的信息量大小（前提自然是状态概率时相等的）：

我们编码前后的状态两侧状态数分别是buckets和(n + 1) ^ k （k只猪猪、n轮实验）
k * log_2(n + 1) >= log_2(buckets) 实际上代表 (n + 1) ^ k >= buckets
根据“鸽笼原理”，我们一定可以把“第i瓶有毒”这一系列事件一个不落地塞到猪猪经过n轮后的存活状况的状态集合中，即1:1，一一映射
不失一般性，我们可以这样描述这种编码：假设有64瓶药水、3只猪、3轮实验，且 (3 + 1) ^ 3 == 64：

我们令 (1, 1, 1) 表示3只小猪都死于第一轮，(1, 1, 2) 表示前两2只小猪死于第一轮、第三只小猪死于第二轮，依次类推
我们把”第一瓶药有毒“ 映射到 (1, 1, 1)，则具体操作是我们把第一瓶药在第一轮同时喂给3只小猪
我们把”第二瓶药有毒“ 映射到 (1, 1, 2)，则具体操作是我们把第二瓶药在第一轮同时喂给2只小猪、第二轮喂给最后一只小猪
我们把”第四瓶药有毒“ 映射到 (1, 1, 4)，这里4代表第三只小猪存活，所以操作比较特殊，第四瓶药水将不喂给第三只小猪
依次类推，我们可以把每瓶药水，在哪一轮，喂给哪只小猪确定下来
根据确定好的顺序，将药水混合
以此我们可以通过观察(x, y, z)最后的取值，根据我们的映射码本，找到究竟是哪瓶药有毒。不妨更进一步，如果这个时候有第65瓶药水，它势必只能和其他某个瓶子共享状态(a, b, c)，那么当实验结果是(a, b, c)出现时，我们则只能确定是两瓶药水中的一瓶有毒，具体是哪一瓶，则无从确定了。

所以，信息量足够的情况下，本题一定有解，且解法不止一种。因此，k * log_2(n + 1) >= log_2(buckets) 也是 “能找到实验方案”的充分条件。

## Solution2

DP
